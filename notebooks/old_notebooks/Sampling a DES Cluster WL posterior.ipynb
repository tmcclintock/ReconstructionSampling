{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster weak lensing\n",
    "\n",
    "Galaxy cluster weak lensing is used to find the total mass inside of a galaxy cluster. Stacked cluster lensing finds the average mass of the stack. In this example we will use the `ARTsampler` to sample the posterior of a 7-dimensional model of a stacked weak lensing analysis. We also have a chain from an original MCMC analysis (with `emcee`) that we can compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import scipy.interpolate as interp\n",
    "import ARTsampler\n",
    "import resampler\n",
    "import cluster_toolkit as ct\n",
    "import scipy.stats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot configurations\n",
    "plt.rc(\"text\", usetex=True)\n",
    "plt.rc(\"font\", size=24, family=\"serif\")\n",
    "plt.rc(\"errorbar\", capsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain shape is   (80000, 7)\n",
      "lnpost shape is  (80000,)\n"
     ]
    }
   ],
   "source": [
    "#Read in the chain and remove burn-in (which I only know is there for this example)\n",
    "Npoints = 32*2500 #32 walkers\n",
    "input_chain = np.loadtxt(\"chain_full_Y1_SAC_z0_l3.FORSAMPLER\")[-Npoints:]\n",
    "lnpost = np.loadtxt(\"likes_full_Y1_SAC_z0_l3.FORSAMPLER\")[-Npoints:]\n",
    "print(\"chain shape is  \", input_chain.shape)\n",
    "print(\"lnpost shape is \", lnpost.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: set up a reconstruction of the posterior\n",
    "For now, we won't use the _true_ posterior, but an ART reconstruction of the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the arguments that we will use\n",
    "#Note: the file was pickled with python 2.7, so we have to encode\n",
    "args = pickle.load(open(\"z0_l3_args.p\", \"rb\"), encoding='latin1')\n",
    "\n",
    "Marr = args[\"Marr\"]\n",
    "bias_arr = args[\"bias_arr\"]\n",
    "args[\"b_spline\"] = interp.interp1d(Marr, bias_arr)\n",
    "\n",
    "def clusterWL_lnpost(params, args):\n",
    "    lM, c, tau, fmis, Am, B0, Rs = params\n",
    "    \n",
    "    if lM < 13.0 or lM > 16.0 or c <= 1.0 or c > 15.0 or Am <= 0.7\\\n",
    "        or tau <= 0.0  or fmis < 0.0 or fmis > 1.0: \n",
    "        return -1e99\n",
    "    if B0 < 0.0 or B0 > 2. or Rs <=0.0 or  Rs > 10.: \n",
    "        return -1e99\n",
    "    LPfmis = (0.32 - fmis)**2/0.05**2\n",
    "    LPtau  = (0.153 - tau)**2/0.03**2\n",
    "    LPA    = (1.021459904697847 - Am)**2/0.0006116071885671028\n",
    "    \n",
    "    lnlike = -0.5*(LPfmis + LPtau + LPA) #Sum of the priors\n",
    "    \n",
    "    h = args[\"h\"]\n",
    "    z = args[\"z\"]\n",
    "    inds = args[\"kept_inds\"]\n",
    "    r = args['r'] #Mpc/h comoving\n",
    "    xi_nl = args['xi_nl']\n",
    "    b_spline = args[\"b_spline\"]\n",
    "    Redges = args[\"Redges\"] #Mpc/h comoving\n",
    "    Rp = args['Rp'] #Mpc/h comoving; projected on the sky\n",
    "    Rlam = args[\"Rlam\"] #Mpc/h comoving\n",
    "    lam = Rlam**5 * 100. #richness\n",
    "    Sigma_crit_inv = args[\"Sigma_crit_inv\"] #pc^2/hMsun comoving\n",
    "    M = 10**lM\n",
    "    Omega_m = 0.3 #CHECK THIS\n",
    "    xi_nfw   = ct.xi.xi_nfw_at_r(r, M, c, Omega_m)\n",
    "    bias = b_spline(M)\n",
    "    xi_2halo = ct.xi.xi_2halo(bias, xi_nl)\n",
    "    xi_hm    = ct.xi.xi_hm(xi_nfw, xi_2halo)\n",
    "    Sigma  = ct.deltasigma.Sigma_at_R(Rp, r, xi_hm, M, c, Omega_m)\n",
    "    DeltaSigma = ct.deltasigma.DeltaSigma_at_R(Rp, Rp, Sigma, M, c, Omega_m)\n",
    "    Rmis = tau*Rlam #Mpc/h\n",
    "    Sigma_mis  = ct.miscentering.Sigma_mis_at_R(Rp, Rp, Sigma, M, c, \n",
    "                                                Omega_m, Rmis, kernel=\"gamma\")\n",
    "    DeltaSigma_mis = ct.miscentering.DeltaSigma_mis_at_R(Rp, Rp, Sigma_mis)\n",
    "    #Note: here, Rs is Mpc physical but Rp is Mpc/h comoving\n",
    "    boost = ct.boostfactors.boost_nfw_at_R(Rp, B0, Rs*h*(1+z))\n",
    "    full_Sigma = (1-fmis)*Sigma + fmis*Sigma_mis\n",
    "    full_DeltaSigma = (1-fmis)*DeltaSigma + fmis*DeltaSigma_mis #miscentering\n",
    "    full_DeltaSigma *= Am #multiplicative bias\n",
    "    full_DeltaSigma /= boost #boost factor\n",
    "    full_DeltaSigma /= (1-full_Sigma*Sigma_crit_inv) #reduced shear\n",
    "    ave_DeltaSigma = ct.averaging.average_profile_in_bins(Redges, Rp, full_DeltaSigma)\n",
    "    \n",
    "    #print(\"ave DeltaSigma is:\\n\\t\",ave_DeltaSigma)\n",
    "    #print(\"SCI: \", Sigma_crit_inv)\n",
    "    #print(\"halo bias: \", bias)\n",
    "    DS_data = args[\"DeltaSigma\"]\n",
    "    Cov = args[\"Cov\"]\n",
    "\n",
    "    #Convert to Msun/pc^2 physical\n",
    "    X = (DS_data - ave_DeltaSigma[inds]*h*(1+z)**2)\n",
    "    lnlike += -0.5*np.dot(X, np.linalg.solve(Cov, X))\n",
    "    \n",
    "    Bp1 = args['Bp1']\n",
    "    Bcov = args['Bcov']\n",
    "    #Note: here, Rs is Mpc physical and Rb is the same\n",
    "    boost_model = ct.boostfactors.boost_nfw_at_R(args['Rb'], B0, Rs)\n",
    "    #print(\"boost model:\", boost_model)\n",
    "    Xb = Bp1 - boost_model\n",
    "    lnlike += -0.5*np.dot(Xb, np.linalg.solve(Bcov, Xb))  \n",
    "    return lnlike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.array([14.662047841051441, 4.5, 0.153, 0.32, 1.02, 0.07, 2.49])\n",
    "print(clusterWL_lnpost(params, args)) #answer = -9.33e+02\n",
    "print(clusterWL_lnpost(input_chain[-1], args))\n",
    "print(lnpost[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: set up the ART sampler\n",
    "In order to set up the sampler, we need to guess the mean and covariance of the target distribution. I know roughly what the means should be, but for the covariance I'm just going to guess a Gaussian covariance with no correlations (i.e. diagonal), even though I know this is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters are: log{10}(M), concentration, tau_{miscentering}, f_{miscentering}\n",
    "#A_m, B_0, R_s (see McClintock et al. 2019 for details)\n",
    "#guess_mean = np.array([14.1, 6., .1, 0.25, 1.02, 0.2, 1.5])\n",
    "#guess_cov = np.diag(np.var(input_chain, 0))\n",
    "prior_volume = np.array([[13., 16.], #log10 M\n",
    "                        [1., 15.], #concentration\n",
    "                        [0.05, 0.4], #tau\n",
    "                        [0.05, 0.6], #fmis\n",
    "                        [0.88, 1.2], #Am\n",
    "                        [0.1, 1.0], #B0\n",
    "                        [0.1, 1.0]]) #Rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_distributions = [scipy.stats.uniform(loc=13, scale=3), \n",
    "                       scipy.stats.uniform(loc=1, scale=14), \n",
    "                       scipy.stats.norm(loc=0.153, scale=0.03), \n",
    "                       scipy.stats.norm(loc=0.32, scale=0.05), \n",
    "                       scipy.stats.norm(loc=1.021459904697847, scale=0.024730693248817406), \n",
    "                       scipy.stats.uniform(loc=0.1, scale=0.9), \n",
    "                       scipy.stats.uniform(loc=0.1, scale=0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = ARTsampler.ARTsampler(prior_distributions, clusterWL_lnpost, args, \n",
    "                                quiet=False, scale=7,\n",
    "                                Ntraining_points=200, Nburn = 1000, Nsteps=2000, max_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_points = sampler.get_training_points()\n",
    "plt.scatter(training_points[:,0], training_points[:,1], c='k', s=10, zorder=1)\n",
    "\n",
    "plt.scatter(input_chain[:5000,0], input_chain[:5000,1], marker='.', c='b', \n",
    "            alpha=0.2, s=0.5, zorder=0)\n",
    "plt.xlabel(r\"$\\log_{10}M_{\\rm 200b}$\")\n",
    "plt.ylabel(r\"$c_{\\rm 200b}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: do some iterations\n",
    "Let's do two iterations to see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "while sampler.single_iteration():\n",
    "    print(\"On iteration {0}\".format(ind))\n",
    "    ind += 1\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_points = sampler.get_training_points()\n",
    "plt.scatter(training_points[:,0], training_points[:,1], c='k', s=10, zorder=1)\n",
    "\n",
    "samples = sampler.get_samples().T\n",
    "plt.scatter(samples[0], samples[1], marker='.', c='r', alpha=0.2, s=0.5, zorder=0)\n",
    "\n",
    "plt.scatter(input_chain[:10000,0], input_chain[:10000,1], marker='.', c='b', alpha=0.2, s=0.5)\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$y$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 2):\n",
    "    training_points = sampler.get_training_points(i)\n",
    "    plt.scatter(training_points[:,0], training_points[:,1], s=10, label=i)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
